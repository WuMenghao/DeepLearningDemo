## 1 向量的范数

向量范数是定义了向量的类似于长度的性质，满足正定，齐次，三角不等式的关系就称作范数。

> 对一个线性空间X，定义泛函 `||•||: X → R` 满足：
> 
> `1. ||x|| ≥ 0 & ||x|| = 0 ↔ x = 0 for x ∈ X` (正定)
>
> `1. ||cx|| ＝ |c|•||x|| for c∈R, x ∈ X` (齐次)
>
> `1. ||a + b|| ≤ ||a|| + ||b|| for a,b ∈ X` (三角不等式)

向量的范数一般又 `L0，L1，L2` 与 `L_infinity`范数，对与向量
`x = (x1,x2,…,xk)`的`Lp`范数为`||x||_p ＝ pow(sum(x1,z2,…,xk), 1/p)`

***

**L0范数**

定义为

> `||x||_0 = sum(|x1|^0,|x2|^0,…,|xk|^0)`

`及非0元素的个数`。

`L0范数`表示向量中非零元素的个数。如果我们使用L0来规划参数向量w，就是希望w
的元素大部分都为0。L0范数的这个属性，使其非常适用于机器学习中的稀疏编码。
`在特征选择中，通过最小化L0范数来寻找最少最优的稀疏特征项`。但是，L0范数的
最小化问题是`NP难题`。而L1范数是L0范数的最优凸近似，它比L0范数要更容易求解
。因此`优化过程将会被准换为更高维的范数（例如L1范数）问题`。

***

**L1范数**

> `||x||_1 = sum(|x1|,|x2|,…,|xk|)`

`L1范数`是向量中各个元素绝对值之和，也被称作`“Lasso regularization”（稀疏规则算子）`。

在机器学习特征选择中，稀疏规则化能够实现特征的自动选择。一般来说，输入向
量X的大部分元素（也就是特征）都是和最终的输出Y没有关系或者不提供任何信息
的，在最小化目标函数的时候考虑这些额外的特征，虽然可以获得更小的训练误差
，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确Y的预测
。`稀疏规则化算子的引入就是为了完成特征自动选择，它会学习地去掉这些没有信
息的特征，也就是把这些特征对应的权重置为0。`

`L0范数与L1范数都可以实现稀疏，而L1范数比L0具有更好的优化求解特性而被广泛
使用。` L0范数本身是特征选择的最直接的方案，但因为之前说到的理由，其不可分
，且很难优化，因此实际应用中我们使用L1来得到L0的最优凸近似。

总结一下上两段的结论就是：`L1范数和L0范数可以实现稀疏，L1因为拥有比L0更好
的优化求解特性而被广泛应用`。这样我们大概知道了可以实现稀疏，但是为什么我
们希望稀疏？让参数稀疏有什么好处呢？这里有两个理由：

> `1）特征选择(Feature Selection)：`
>
> 大家希望稀疏规则化的一个关键原因在于它能实现特征的自动选择。一般来说，X
> 的大部分元素（也就是特征）都是和最终的输出没有关系或者不提供任何信息的，
> 在最小化目标函数的时候考虑这些额外的特征，虽然可以获得更小的训练误差，但
> 在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确的预测。稀
> 疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些
> 没有信息的特征，也就是把这些特征对应的权重置为0。
>
> `2）可解释性(Interpretability)：`
>
> 另一个青睐于稀疏的理由是，模型更容易解释。例如`患某种病的概率是y，然后我
> 们收集到的数据x是1000维`的，也就是我们需要寻找这1000种因素到底是怎么影响
> 患上这种病的概率的。假设这是个回归模型：
>
> `y = ∑ wi * xi + b (i[1-1000])`

> 当然了，为了让y限定在`[0,1]`的范围，一般还得加个`Logistic函数`。
> 通过学习，`如果最后学习到的 w 就只有很少的非零元素，例如只有5个非零的
> wi ，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨
> 大的，决策性的`。也就是说，`患不患这种病只和这5个因素有关`，那医生就好分
> 析多了。但如果1000个`wi`都非0，医生面对这1000种因素，累觉不爱。

***

**L2范数**

> `||x||_2 = square(sum(|x1|^2,|x2|^2,…,|xk|^2))`

`L2范数`是我们最常用的范数了，我们用的最多的度量距离`欧式距离就是一种L2范数`
。在回归里面，有人把有它的回归叫`岭回归(Ridge Regression)`，有人也叫它
`权值衰减(Weight Decay)`。`L2范数在机器学习中有一个非常重要的作用，它被
用来改善机器学习里面一个非常重要的问题：过拟合`。

1. 为什么L2范数可以防止过拟合？回答这个问题之前，我们得先看看L2范数实际上是什么：

> `L2范数是指向量各元素的平方和求和开平方根`。我们让L2范数的规则项最小，可以使
得每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，这
是有很大区别的。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合
现象。为什么越小的参数说明模型越简单？因为限制了参数很小，实际上就是限制了多
项式某些分量的影响很小，这样就相当于减少了参数个数。

> 总结一下：`通过L2范数，我们可以实现对模型空间的限制，从而在一定程度上避免了过拟合`。


2. L2范数的好处是什么呢？

> 1). 学习理论的角度：
> 
>从学习理论的角度来说，L2范数可以防止过拟合，提升模型能力。
>
> 2).优化计算的角度：
>
> 从优化或者数值计算的角度来说，L2范数有助于处理 Condition Number 不好的情况
> 下矩阵求逆困难的问题。