## 1 向量的范数

向量范数是定义了向量的类似于长度的性质，满足正定，齐次，三角不等式的关系就称作范数。

> 对一个线性空间X，定义泛函 `||•||: X → R` 满足：
> 
> `1. ||x|| ≥ 0 & ||x|| = 0 ↔ x = 0 for x ∈ X` (正定)
>
> `1. ||cx|| ＝ |c|•||x|| for c∈R, x ∈ X` (齐次)
>
> `1. ||a + b|| ≤ ||a|| + ||b|| for a,b ∈ X` (三角不等式)

向量的范数一般又 `L0，L1，L2` 与 `L_infinity`范数，对与向量
`x = (x1,x2,…,xk)`的`Lp`范数为`||x||_p ＝ pow(sum(x1,z2,…,xk), 1/p)`

**L0范数**

定义为

> `||x||_0 = sum(|x1|^0,|x2|^0,…,|xk|^0)`

`及非0元素的个数`。

L0范数表示向量中非零元素的个数。如果我们使用L0来规划参数向量w，就是希望w
的元素大部分都为0。L0范数的这个属性，使其非常适用于机器学习中的稀疏编码。
`在特征选择中，通过最小化L0范数来寻找最少最优的稀疏特征项`。但是，L0范数的
最小化问题是`NP难题`。而L1范数是L0范数的最优凸近似，它比L0范数要更容易求解
。因此优化过程将会被准换为更高维的范数（例如L1范数）问题。

**L1范数**

> `||x||_1 = sum(|x1|,|x2|,…,|xk|)`

L1范数是向量中各个元素绝对值之和，也被称作“Lasso regularization”（稀疏规则算子）。

在机器学习特征选择中，稀疏规则化能够实现特征的自动选择。一般来说，输入向
量X的大部分元素（也就是特征）都是和最终的输出Y没有关系或者不提供任何信息
的，在最小化目标函数的时候考虑这些额外的特征，虽然可以获得更小的训练误差
，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确Y的预测
。稀疏规则化算子的引入就是为了完成特征自动选择，它会学习地去掉这些没有信
息的特征，也就是把这些特征对应的权重置为0。

L0范数与L1范数都可以实现稀疏，而L1范数比L0具有更好的优化求解特性而被广泛
使用。 L0范数本身是特征选择的最直接的方案，但因为之前说到的理由，其不可分
，且很难优化，因此实际应用中我们使用L1来得到L0的最优凸近似。

总结一下上两段的结论就是：L1范数和L0范数可以实现稀疏，L1因为拥有比L0更好
的优化求解特性而被广泛应用。这样我们大概知道了可以实现稀疏，但是为什么我
们希望稀疏？让参数稀疏有什么好处呢？这里有两个理由：

> 1）特征选择(Feature Selection)：
>
> 大家希望稀疏规则化的一个关键原因在于它能实现特征的自动选择。一般来说，X
> 的大部分元素（也就是特征）都是和最终的输出没有关系或者不提供任何信息的，
> 在最小化目标函数的时候考虑这些额外的特征，虽然可以获得更小的训练误差，但
> 在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确的预测。稀
> 疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些
> 没有信息的特征，也就是把这些特征对应的权重置为0。
>
> 2）可解释性(Interpretability)：
>
> 另一个青睐于稀疏的理由是，模型更容易解释。例如患某种病的概率是y，然后我
> 们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响
> 患上这种病的概率的。假设这是个回归模型：
>
> `y = ∑ wi * xi + b`